<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Vision2UI</title>
    <link rel="stylesheet" href="./index.css" />
</head>

<body>
    <div class="header">
        <div class="logo">Vision2UI</div>
        <div class="space"></div>
        <div class="naviList">
            <a href="#overview">
                <div class="navi">Overview</div>
            </a>
            <a href="https://arxiv.org/abs/2404.06369">
                <div class="navi">Paper</div>
            </a>
            <a href="https://huggingface.co/datasets/xcodemind/vision2ui">
                <div class="navi">Dataset</div>
            </a>
            <a href="https://huggingface.co/datasets/xcodemind/vision2ui_benchmark">
                <div class="navi">Benchmark</div>
            </a>
            <a href="https://github.com/anonymouscodeeee/repo1">
                <div class="navi">Github</div>
            </a>
        </div>
    </div>
    <div class="poster">
        <img src="./images/comparision.png" alt="" class="image" />
        <div class="title">
            <span style="color: #2869d1; text-shadow: 5px 5px 0 rgba(0, 0, 0, 0.1)">Vision2UI</span
        >
        <br />
        A Real-World Dataset <br />
        for Code Generation <br />
        from UI Designs<br />
        with Layout <br />
      </div>
    </div>
    <div class="main">
      <div class="block" id="overview">
        <h1 class="title">Introduction</h1>
        <div class="content">
          <img src="./images/workflow.png" alt="" />
          <p>
            In this paper, we pioneer the curation of a novel dataset for UI
            code generation, which we term Vision2UI. Specifically, there are
            three-fold major contributions:
          </p>
          <p>
            <b>A Real-World Dataset.</b>
            To the best of our knowledge, Vision2UI is the first real-world and
            large-scale dataset tailored to empower MLLMs for training and
            testing purposes in the domain of generating webpage code from
            high-fidelity images.
          </p>
          <p>
            <b>A SOTA MLLM.</b>
            We finetune an MLLM, named UICoder, on our training set and evaluate
            it with a comprehensive array of experiments.The experimental
            results demonstrate the dataset's efficacy in enabling MLLMs to
            perform automatically code generation from UI design images. We also
            propose a novel metric, termed TreeBLEU, to assess the structural
            similarity between the generated webpages' Document Object Model
            (DOM) trees and the ground truth.
          </p>
        </div>
      </div>
      <div class="block">
        <h1 class="title">Datasets</h1>
        <div class="content">
          <p>
            A statistical comparison between our dataset and all the publicly
            available datasets.
          </p>
          <img src="./images/s1.png" alt="" />
          <p>
            Representative screenshots of webpages in Vision2UI and other
            datasets. The images in our dataset demonstrate superior diversity
            in terms of structure and elements compared to other datasets.
          </p>
          <img src="./images/comparision.png" alt="" style="width: 75%" />
        </div>
      </div>
      <div class="block">
        <h1 class="title">Evaluation Results</h1>
        <div class="content">
          <img src="./images/eval.png" alt="" />
          <p>
            Experimental results demonstrate that our model significantly
            surpasses all baselines on the TreeBLEU across all three test
            sets. Given that our test sets are manually selected from the
            real world, this result also suggests that our model is better at
            capturing the structural information of UI design images when
            generating real-world webpage code. On the CLIP Similarity, our
            model is better or on par with GPT-4V. On the Visual Score, our
            model either exceeds other models or ranks second to Design2Code,
            yet noticeably superior to other baselines. The results on these two
            metrics indicate that our model outperforms other baselines or is
            equal to the best-performing baseline, in terms of overall and
            low-level visual measurements. Furthermore, comparative experiments
            reveal that the base model, namely Pix2Struct, significantly
            outperforms its own performance when finetuned on our training
            dataset as opposed to the WebSight dataset. These experimental
            results suggest that our proposed training dataset can
            effectively unleash the potential of MLLMs in code generation.
          </p>
        </div>
      </div>
    </div>
  </body>
</html>